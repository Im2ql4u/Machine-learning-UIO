{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a540456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch(xTrain, yTrain, batches,batch_size, i)\n",
    "    \n",
    "    #println((batches,i, a, b))\n",
    "    if length(size(yTrain))==1\n",
    "        a, b = Int(batch_size*(i-1)+1), min(batch_size * i, length(yTrain))\n",
    "        if i==batches\n",
    "            input, label = xTrain[a:end,:], yTrain[a:end]\n",
    "        else\n",
    "            input, label = xTrain[a:b,:], yTrain[a:b]\n",
    "        end \n",
    "    elseif length(yTrain[:,1])==1\n",
    "        a, b = Int(batch_size*(i-1)+1), min(batch_size * i, length(yTrain[:,1]))\n",
    "        input, label = xTrain[a:b,:], yTrain[:,a:b]\n",
    "    elseif length(yTrain[:,1])==10\n",
    "        a, b = Int(batch_size*(i-1)+1), min(batch_size * i, length(yTrain[1,:]))\n",
    "        input, label = xTrain[a:b,:], yTrain[:,a:b]\n",
    "    else\n",
    "        #println(\"here\")\n",
    "        a, b = Int(batch_size*(i-1)+1), min(batch_size * i, length(yTrain[:,1]))\n",
    "        input, label = xTrain[a:b,:], yTrain[a:b,:]\n",
    "        #display(yTrain)\n",
    "    end\n",
    "    return input, label\n",
    "end\n",
    "\n",
    "function accuracy_score(y_true::Matrix{Int}, y_pred::Matrix{Float64})\n",
    "    # Find the index of the maximum value for each column in y_true and y_pred\n",
    "    true_indices = argmax(y_true, dims=1)\n",
    "    pred_indices = argmax(y_pred, dims=1)\n",
    "    \n",
    "    # Flatten the result to convert from a 1xN matrix to a vector\n",
    "    true_indices = vec(true_indices)\n",
    "    pred_indices = vec(pred_indices)\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = sum(true_indices .== pred_indices)\n",
    "    \n",
    "    # Calculate the accuracy as a percentage\n",
    "    accuracy = correct_predictions / length(true_indices)\n",
    "    \n",
    "    return accuracy\n",
    "end\n",
    "function initialize(method, layers, learning_rate, num_batches,batch_size;γ=0.9, ρ=0.9, ρ_1 = 0.9, ρ_2=0.999)\n",
    "    if method==SGD!\n",
    "        return (learning_rate, num_batches,batch_size)\n",
    "\n",
    "    elseif method==Momentum!\n",
    "        vW = [zeros(size(W)) for (W, b) in layers]\n",
    "        vb = [zeros(size(b)) for (W, b) in layers]\n",
    "        return (vW, vb, learning_rate, γ, num_batches,batch_size)\n",
    "    \n",
    "    elseif method==AdaGrad!\n",
    "        rW = [zeros(size(W)) for (W, b) in layers]\n",
    "        rb = [zeros(size(b)) for (W, b) in layers]\n",
    "        return (rW, rb, learning_rate, num_batches,batch_size)\n",
    "    elseif method==RMSProp!\n",
    "        rW = [zeros(size(W)) for (W, b) in layers]\n",
    "        rb = [zeros(size(b)) for (W, b) in layers]\n",
    "        return (rW, rb, ρ, learning_rate, num_batches,batch_size)\n",
    "    elseif method==ADAM!\n",
    "        rW = [zeros(size(W)) for (W, b) in layers]\n",
    "        rb = [zeros(size(b)) for (W, b) in layers]\n",
    "        sW = [zeros(size(W)) for (W, b) in layers]\n",
    "        sb = [zeros(size(b)) for (W, b) in layers]\n",
    "        t=0\n",
    "        return (ρ_1, ρ_2, rW, sW, rb, sb, t, learning_rate, num_batches,batch_size)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0e9524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Train(xdata, ydata, layers, activation_funcs, learning_rate=0.001, epochs=100; batch_size, optimizer, loss, evaluate)\n",
    "    xTrain, xTest, yTrain, yTest = SplitData(xdata, ydata, 0.7)\n",
    "    num_batches =Int(round(length(yTrain[:,1])/batch_size))\n",
    "    params = initialize(optimizer, layers, learning_rate, num_batches)\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        (layers, params) = optimizer(xTrain,yTrain, layers,  activation_funcs,loss, params)\n",
    "        if epoch%10==0\n",
    "            println(\"Epoch:  \", epoch, \"  Loss: \", evaluate(yTest, feed_forward_batched(layers, xTest, activation_funcs;backprop=false)))\n",
    "        end\n",
    "    end\n",
    "    return layers\n",
    "end\n",
    "\n",
    "function SGD!(xTrain, yTrain, layers, activation_funcs, loss_func, params)\n",
    "    (η,num_batches,batch_size) = params\n",
    "    #γ = 0.9\n",
    "    for i in 1:num_batches\n",
    "        input, target = batch(xTrain, yTrain, num_batches,batch_size, i)\n",
    "\n",
    "        grads = Zygote.gradient(layers -> loss_func(input, layers, activation_funcs, target), layers)[1]\n",
    "        for ((W, b), (W_g, b_g)) in zip(layers, grads)\n",
    "            W .-=η.*W_g\n",
    "            b .-=η.*b_g\n",
    "        end\n",
    "    end\n",
    "    params = (η,num_batches,batch_size)\n",
    "    return (layers, params)\n",
    "end\n",
    "                                                            \n",
    "function Momentum!(xTrain, yTrain, layers, activation_funcs, loss_func, params)\n",
    "    (vW, vb, η, γ, num_batches,batch_size) = params\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        # Get the current batch of data\n",
    "        input, target = batch(xTrain, yTrain, num_batches,batch_size, i)\n",
    "        # Calculate the gradients\n",
    "        grads = Zygote.gradient(layers -> loss_func(input, layers, activation_funcs, target), layers)[1]\n",
    "\n",
    "        # Update weights and biases using momentum\n",
    "        for j in 1:length(layers)\n",
    "            W, b = layers[j]\n",
    "            W_g, b_g = grads[j]\n",
    "            \n",
    "            # Update velocity terms\n",
    "            vW[j] = γ .* vW[j] .+ η .* W_g\n",
    "            vb[j] = γ .* vb[j] .+ η .* b_g\n",
    "\n",
    "            # Update parameters\n",
    "            layers[j] = (W .- vW[j], b .- vb[j])  # In-place update for weights and biases\n",
    "        end\n",
    "    end\n",
    "    params = (vW, vb, η, γ, num_batches,batch_size)\n",
    "\n",
    "    return (layers, params)\n",
    "end\n",
    "\n",
    "function AdaGrad!(xTrain, yTrain, layers, activation_funcs, loss_func, params)\n",
    "    (rW, rb, η, num_batches,batch_size) = params\n",
    "    δ=1e-7\n",
    "    # Iterate over batches\n",
    "    for i in 1:num_batches\n",
    "        # Get the current batch of data\n",
    "        input, target = batch(xTrain, yTrain, num_batches,batch_size, i)\n",
    "        grads = Zygote.gradient(layers -> loss_func(input, layers, activation_funcs, target), layers)[1]\n",
    "        # Update weights and biases using momentum\n",
    "        for j in 1:length(layers)\n",
    "            W, b = layers[j]\n",
    "            W_g, b_g = grads[j]\n",
    "            rW[j] = rW[j] .+ (W_g .* W_g)\n",
    "            rb[j] = rb[j] .+ (b_g .* b_g)\n",
    "            # Update velocity terms\n",
    "            Δθ_W = @. η/(δ+√rW[j])* W_g\n",
    "            Δθ_b = @. η/(δ+√rb[j])* b_g\n",
    "\n",
    "            # Update parameters\n",
    "            layers[j] = (W .- Δθ_W, b .- Δθ_b)  # In-place update for weights and biases\n",
    "        end\n",
    "    end\n",
    "    params = (rW, rb, η, num_batches,batch_size)\n",
    "    return (layers, params)\n",
    "end\n",
    "\n",
    "function RMSProp!(xTrain, yTrain, layers, activation_funcs, loss_func, params)\n",
    "    (rW, rb, ρ, η, num_batches,batch_size) = params\n",
    "    δ=1e-7\n",
    "    # Iterate over batches\n",
    "    for i in 1:num_batches\n",
    "        # Get the current batch of data\n",
    "        input, target = batch(xTrain, yTrain, num_batches,batch_size, i)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        grads = Zygote.gradient(layers -> loss_func(input, layers, activation_funcs, target), layers)[1]\n",
    "        \n",
    "        # Update weights and biases using momentum\n",
    "        for j in 1:length(layers)\n",
    "            W, b = layers[j]\n",
    "            W_g, b_g = grads[j]\n",
    "            \n",
    "            rW[j] = @. ρ*rW[j] + (1-ρ)*(W_g .* W_g)\n",
    "            rb[j] = @. ρ*rb[j] + (1-ρ)*(b_g .* b_g)\n",
    "            # Update velocity terms\n",
    "            Δθ_W = @. η/(δ+√rW[j]) .* W_g\n",
    "            Δθ_b = @. η/(δ+√rb[j]) .* b_g\n",
    "\n",
    "            # Update parameters\n",
    "            layers[j] = (W .- Δθ_W, b .- Δθ_b)  # In-place update for weights and biases\n",
    "        end\n",
    "    end\n",
    "    params = (rW, rb, ρ, η, num_batches,batch_size)\n",
    "    return (layers, params)\n",
    "end\n",
    "\n",
    "function ADAM!(xTrain, yTrain, layers, activation_funcs, loss_func, params)\n",
    "    (ρ_1, ρ_2, rW, sW, rb, sb, t, η, num_batches, batch_size) = params\n",
    "    #println(params)\n",
    "    δ=1e-7\n",
    "    # Iterate over batches\n",
    "\n",
    "    for i in 1:num_batches\n",
    "        # Get the current batch of data\n",
    "        input, target = batch(xTrain, yTrain, num_batches,batch_size, i)\n",
    "        # Calculate the gradients\n",
    "        grads = Zygote.gradient(layers -> loss_func(input, layers, activation_funcs, target), layers)[1]\n",
    "        \n",
    "        # Update weights and biases using momentum\n",
    "        t=t+1\n",
    "        #println(t)\n",
    "        for j in 1:length(layers)\n",
    "            W, b = layers[j]\n",
    "            W_g, b_g = grads[j]\n",
    "            \n",
    "            sW[j] .= ρ_1.*sW[j] .+ (1-ρ_1)*W_g\n",
    "            rW[j] .= ρ_2.*rW[j] .+ (1-ρ_2)*(W_g.*W_g)\n",
    "            \n",
    "            sb[j] .= ρ_1.*sb[j] .+ (1-ρ_1)*b_g\n",
    "            rb[j] .= ρ_2.*rb[j] .+ (1-ρ_2)*(b_g.*b_g)\n",
    "            \n",
    "            sW_temp = sW[j] ./(1-ρ_1^t)\n",
    "            rW_temp = rW[j] ./(1-ρ_2^t)\n",
    "            sb_temp = sb[j] ./(1-ρ_1^t)\n",
    "            rb_temp = rb[j] ./(1-ρ_2^t)\n",
    "\n",
    "            # Update velocity terms\n",
    "            Δθ_W = @. η*sW_temp/(δ+√rW_temp)\n",
    "            Δθ_b = @. η*sb_temp/(δ+√rb_temp)\n",
    "\n",
    "            # Update parameters\n",
    "            layers[j] = (W .- Δθ_W, b .- Δθ_b)  # In-place update for weights and biases\n",
    "        end\n",
    "    end\n",
    "    params=(ρ_1, ρ_2, rW, sW, rb, sb, t, η, num_batches,batch_size)\n",
    "    return (layers, params)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4417fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
