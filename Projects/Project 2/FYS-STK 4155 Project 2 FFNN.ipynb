{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create layers\n",
    "function create_layers_batch(network_input_size, layer_output_sizes)\n",
    "    input_size = network_input_size\n",
    "    layers = []\n",
    "    for output_size in layer_output_sizes\n",
    "        W = randn(output_size, input_size)  # W: (output_size, input_size)\n",
    "        b = randn(output_size)\n",
    "        push!(layers, (W, b))\n",
    "        input_size = output_size\n",
    "    end\n",
    "    return layers\n",
    "end\n",
    "\n",
    "function feed_forward_batched(layers, input, activation_functions; backprop=false)\n",
    "    if length(size(input)) == 1\n",
    "        input = reshape(input, :, 1)  # Ensure input is a column vector\n",
    "    end\n",
    "    if backprop\n",
    "        Back = []\n",
    "        a = input\n",
    "        push!(Back, (a, nothing))  # Store input activations; z is nothing\n",
    "        for ((W, b), func) in zip(layers, activation_functions)\n",
    "            z = a * W' .+ b'  # Compute pre-activation\n",
    "            a = func(z)       # Compute activation\n",
    "            push!(Back, (a, z))  # Store activation after current layer and pre-activation\n",
    "        end\n",
    "\n",
    "        return a, Back\n",
    "    else\n",
    "        a = input\n",
    "\n",
    "        for ((W, b), func) in zip(layers, activation_functions)\n",
    "            z = a * W' .+ b'  # Compute pre-activation\n",
    "            a = func(z)       # Compute activation\n",
    "        end\n",
    "        return a\n",
    "    end\n",
    "end\n",
    "ReLU(z) = max.(0, z)\n",
    "\n",
    "function Sigmoid(z)\n",
    "    return 1 ./ (1 .+ exp.(-z))\n",
    "end\n",
    "function Softmax(z)\n",
    "    max_z = maximum(z, dims=2)  # Maximum along the class dimension\n",
    "    e_z = exp.(z .- max_z)      # Subtract max for numerical stability\n",
    "    sum_e_z = sum(e_z, dims=2)  # Sum across the class dimension\n",
    "    return e_z ./ sum_e_z       # Normalize by sum of exponentials\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e51a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusionMatrix (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function BackwardsPropagation(layers, y, backs, activation_functions)\n",
    "    batch_size = size(y, 1)\n",
    "    L = length(layers)\n",
    "    δ = Vector{Any}(undef, L)  # List to store δ_l at each layer\n",
    "    gradients = Vector{Any}(undef, L)\n",
    "    η = 0.001  # Learning rate\n",
    "\n",
    "    # Compute delta for the output layer (for binary cross-entropy with sigmoid)\n",
    "    (a_L, z_L) = backs[end]  # a_L is the final output, z_L is the pre-activation\n",
    "    δ_L = activation_derivative(a_L, activation_functions[L])  # δ_L = p - y, where p is sigmoid(a_L)\n",
    "    δ[L] = δ_L\n",
    "\n",
    "    # Backpropagate the error through the hidden layers\n",
    "    for l in (L-1):-1:1\n",
    "        # Get weights and delta from the next layer\n",
    "        W_next, _ = layers[l + 1]\n",
    "        δ_next = δ[l + 1]\n",
    "\n",
    "        # Compute delta for the current layer\n",
    "        δ_temp = δ_next * W_next  # Propagate delta back\n",
    "\n",
    "        # Compute derivative of activation function\n",
    "        (a_l, z_l) = backs[l+1]  # z_l corresponds to layer l\n",
    "        σ_prime = activation_derivative(a_l, activation_functions[l])  # Derivative of sigmoid: a_l * (1 - a_l)\n",
    "\n",
    "        # Compute delta for the current layer\n",
    "        δ_l = δ_temp .* σ_prime  # Element-wise multiplication\n",
    "        δ[l] = δ_l\n",
    "    end\n",
    "\n",
    "    # Update weights and biases\n",
    "    for l in 1:L\n",
    "        W_l, b_l = layers[l]\n",
    "        (a_prev, _) = backs[l]  # Activations from the previous layer\n",
    "        δ_l = δ[l]\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_W = (a_prev' * δ_l)' / batch_size  # Gradient for weights\n",
    "        grad_b = (mean(δ_l, dims=1)')  # Gradient for biases\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W_l -= η * grad_W\n",
    "        b_l -= η * grad_b\n",
    "\n",
    "        # Update the layer in the list\n",
    "        layers[l] = (W_l, b_l)\n",
    "        gradients[l] = (grad_W, grad_b)\n",
    "    end\n",
    "\n",
    "    return layers\n",
    "end\n",
    "function activation_derivative(a_l, activation_func)\n",
    "    if activation_func == Sigmoid\n",
    "        # Derivative of sigmoid: a_l * (1 - a_l)\n",
    "        return a_l .* (1 .- a_l)\n",
    "    elseif activation_func == ReLU\n",
    "        # Derivative of ReLU: 1 if a_l > 0, else 0\n",
    "        return Float64(a_l .> 0)\n",
    "    elseif activation_func == Softmax\n",
    "        # Softmax derivative is handled with cross-entropy in the output layer\n",
    "        # No need for a separate derivative here\n",
    "        return a_l\n",
    "    else\n",
    "        error(\"Unknown activation function: $activation_func\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function ConfusionMatrix(model, input, target, act_func=nothing)\n",
    "    # Obtain predictions from the model\n",
    "    predictions = act_func == nothing ? model(input) : feed_forward_batched(model, input, activation_functions; backprop=false)\n",
    "\n",
    "    # Determine if targets are one-hot encoded\n",
    "    if ndims(target) == 2 && size(target, 1) > 1 && size(target, 2) != 1\n",
    "        # One-hot encoded targets (e.g., MNIST)\n",
    "        target_labels = vec(map(argmax, eachcol(target)))\n",
    "        predicted_labels = size(predictions, 1) >= size(predictions, 2) ? vec(map(argmax, eachrow(predictions))) : vec(map(argmax, eachcol(predictions)))\n",
    "    else\n",
    "        # Non-one-hot encoded targets (e.g., Wisconsin dataset)\n",
    "        target_labels = vec(target)\n",
    "        println(size(predictions, 2))\n",
    "        \n",
    "        if size(predictions, 1) == 1\n",
    "            predicted_labels = predictions .>= 0.5\n",
    "            predicted_labels = vec(predicted_labels)\n",
    "        elseif size(predictions, 2) == 1\n",
    "            predicted_labels = predictions .>= 0.5\n",
    "            predicted_labels = vec(predicted_labels)\n",
    "        else\n",
    "            predicted_labels = vec(map(argmax, eachcol(predictions)))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Ensure labels are integers starting from 1\n",
    "    unique_labels = sort(unique(vcat(target_labels)))\n",
    "    label_to_index = Dict(label => idx for (idx, label) in enumerate(unique_labels))\n",
    "\n",
    "    indexed_target_labels = [label_to_index[label] for label in target_labels]\n",
    "    indexed_predicted_labels = [label_to_index[label] for label in predicted_labels]\n",
    "    #display(indexed_predicted_labels)\n",
    "    num_classes = length(unique_labels)\n",
    "    cm = zeros(Int, num_classes, num_classes)\n",
    "\n",
    "    # Populate the confusion matrix\n",
    "    for (t, p) in zip(indexed_target_labels, indexed_predicted_labels)\n",
    "        cm[t, num_classes+1-p] += 1\n",
    "    end\n",
    "    \n",
    "    # Normalize the confusion matrix to show percentages\n",
    "    cm_percentage = zeros(Float64, num_classes, num_classes)\n",
    "    for i in 1:num_classes\n",
    "        total_in_class = sum(cm[i, :])  # Total instances of class i\n",
    "        if total_in_class > 0\n",
    "            cm_percentage[i, :] = cm[i, :] / total_in_class #* 100  # Convert to percentage\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return cm_percentage, unique_labels\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
