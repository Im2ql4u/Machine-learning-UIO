{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c241ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Franke(x,y)\n",
    "    p1 = 3/4*ℯ^(-(9x-2)^2/4-(9y-2)^2/4)\n",
    "    p2 = 3/4*ℯ^(-(9x+1)^2/49-(9y+1)^2/10)\n",
    "    p3 = 1/2*ℯ^(-(9x-7)^2/4-(9y-3)^2/4)\n",
    "    p4 = -1/5*ℯ^(-(9x-4)^2-(9y-7)^2)\n",
    "    return p1+p2+p3+p4\n",
    "end\n",
    "\"\"\"\n",
    "    SplitData2D(xData, yData, zData, train_size)\n",
    "\n",
    "Splits data with given split: train_size\n",
    "\n",
    "\n",
    "# Input\n",
    " - xData::Array:        Data in X  \n",
    " - yData::Array:        Data in Y\n",
    " - zData::Array:        Data in Z\n",
    " - train_size::Float64: Proportion of data to be given as trainingdata\n",
    "\n",
    "\"\"\"\n",
    "function SplitData2D(xData::Array, yData::Array, zData::Array, train_size::Float64)\n",
    "    # Assumes xData and yData are vectors, and zData is a matrix representing the grid\n",
    "\n",
    "    # Flatten the Z matrix and corresponding X, Y arrays\n",
    "    xGrid = repeat(xData, inner=length(yData))\n",
    "    yGrid = repeat(yData, outer=length(xData))\n",
    "    zVector = vec(zData)\n",
    "\n",
    "    # Number of total samples\n",
    "    N = length(zVector)\n",
    "    \n",
    "    # Determine number of training samples\n",
    "    NumTrain = Int(round(N * train_size))\n",
    "    \n",
    "    # Randomly sample indices for training data\n",
    "    train_indices = sample(1:N, NumTrain; replace=false)\n",
    "    \n",
    "    # Determine the test indices\n",
    "    all_indices = collect(1:N)\n",
    "    test_indices = [i for i in all_indices if i ∉ train_indices]\n",
    "    \n",
    "    # Create training and testing datasets\n",
    "    xTrain = xGrid[train_indices]\n",
    "    yTrain = yGrid[train_indices]\n",
    "    zTrain = zVector[train_indices]\n",
    "    \n",
    "    xTest = xGrid[test_indices]\n",
    "    yTest = yGrid[test_indices]\n",
    "    zTest = zVector[test_indices]\n",
    "    \n",
    "    return xTrain, yTrain, zTrain, xTest, yTest, zTest\n",
    "end\n",
    "\"\"\"\n",
    "    DataProjector(x, y, z, numDegrees;train, normalize)\n",
    "\n",
    "This function Projects the Data from the type we generate to the type we need for Regression.\n",
    "\n",
    "\n",
    "# Input\n",
    " - x:        Data in X  \n",
    " - y:        Data in Y\n",
    " - z:        Data in Z\n",
    " - numDegrees: The number of orders of the polynomial\n",
    " - train: Whether or not the data should be handeled as training data\n",
    " - normalize: Whether or not the DesignMatrix should be normalized\n",
    "\"\"\"\n",
    "function DataProjector(x, y, z, numDegrees;train=true, normalize=false)\n",
    "\n",
    "    if !train\n",
    "        #We flatten the data\n",
    "        numSamples = length(x)*length(y)\n",
    "        xData = repeat(x, inner=length(y))  # Repeat xData \n",
    "        yData = repeat(y, outer=length(x))  # Repeat yData \n",
    "        zData = vec(z) #Flattened Z data\n",
    "    else\n",
    "        numSamples = length(z)\n",
    "        xData=x\n",
    "        yData=y\n",
    "        zData=z\n",
    "    end\n",
    "     #NxN\n",
    "    numFeatures = (numDegrees + 1) * (numDegrees + 2) ÷ 2  # Number of polynomial terms\n",
    "    DesignMatrix = zeros(numSamples, numFeatures)\n",
    "    column=1\n",
    "    for i in 0:numDegrees\n",
    "        for j in 0:(numDegrees-i)\n",
    "            DesignMatrix[:,column] .= (xData .^ i) .* (yData .^ j)\n",
    "            column+=1\n",
    "        end\n",
    "    end\n",
    "    if normalize\n",
    "        col_std = zeros(column-1)\n",
    "        col_mean = zeros(column-1)\n",
    "        for j in 1:(column-1)\n",
    "            col_mean[j] = mean(DesignMatrix[:, j])\n",
    "            col_std[j] = std(DesignMatrix[:, j])\n",
    "\n",
    "            # Avoid dividing by zero if the standard deviation is zero (constant column)\n",
    "            if col_std[j] != 0\n",
    "                DesignMatrix[:, j] = (DesignMatrix[:, j] .- col_mean[j]) ./ col_std[j]\n",
    "            else\n",
    "                DesignMatrix[:, j] = DesignMatrix[:, j]  # No standardization if std is 0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    if train\n",
    "        if !normalize\n",
    "            return DesignMatrix\n",
    "        else\n",
    "            return DesignMatrix, col_std, col_mean\n",
    "        end\n",
    "    else\n",
    "        if !normalize\n",
    "            return DesignMatrix, zData\n",
    "        else\n",
    "            return DesignMatrix, zData, col_std, col_mean\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function LassoReg(DesignMatrix, zData, γ)\n",
    "    (T, K) = (size(DesignMatrix, 1), size(DesignMatrix, 2))\n",
    "    Q = DesignMatrix'DesignMatrix #/ T\n",
    "    c = DesignMatrix'zData #/ T                      #c'b = Y'X*b\n",
    "\n",
    "    b = Variable(K)              #define variables to optimize over\n",
    "    L1 = quadform(b, Q)            #b'Q*b\n",
    "    L2 = dot(c, b)                 #c'b\n",
    "    L3 = norm(b, 1)                #sum(|b|)\n",
    "\n",
    "    # u'u/T + γ*sum(|b|) where u = Y-Xb\n",
    "    problem = minimize(L1 - 2 * L2 + γ * L3)\n",
    "\n",
    "    solve!(problem, SCS.Optimizer; silent = true)\n",
    "    problem.status == Convex.MOI.OPTIMAL ? beta = vec(Convex.evaluate(b)) : beta = NaN\n",
    "\n",
    "    return beta\n",
    "end\n",
    "\"\"\"\n",
    "    Regression(DesignMatrix, zData, Method; λ)\n",
    "\n",
    "Preforms a Regression with a DesignMatrix, zData and a given method\n",
    "\n",
    "\n",
    "# Input\n",
    " - DesignMatrix: The designmatrix for the regression\n",
    " - zData:        The z data for the regression\n",
    " - Method:       The method, either; OLS, Ridge or Lasso\n",
    " - λ:            Parameter for Ridge and Lasso regression\n",
    "\"\"\"\n",
    "function Regression(DesignMatrix, zData, Method; λ=0)\n",
    "    if Method==\"OLS\"\n",
    "        Hessian = Transpose(DesignMatrix)*DesignMatrix\n",
    "        beta = inv(Hessian)*Transpose(DesignMatrix)*zData\n",
    "        \n",
    "    elseif Method==\"Ridge\"\n",
    "        Hessian = Transpose(DesignMatrix)*DesignMatrix\n",
    "        beta = inv(Hessian+λ*I)*Transpose(DesignMatrix)*zData\n",
    "        \n",
    "    elseif Method==\"Lasso\"\n",
    "        beta = LassoReg(DesignMatrix, zData, λ)\n",
    "        \n",
    "    else\n",
    "        @warn \"Method Undefined\" \n",
    "        println(\"Expected: OLS, Lasso or Ridge, but got: \", Method)\n",
    "        return\n",
    "    end\n",
    "    return beta\n",
    "end\n",
    "\n",
    "function MSE(DesignMatrix, z, beta; λ=0, γ=0)\n",
    "    N = length(z)\n",
    "    MSE = 1/N*sum((z .- DesignMatrix*beta).^2)+λ*(Transpose(beta)*beta) + γ*sum(abs.(beta))\n",
    "    return MSE\n",
    "end\n",
    "\n",
    "function R2(DesignMatrix, zData, beta)\n",
    "    N = length(zData)\n",
    "    z_avg = 1/N*sum(zData)\n",
    "    R2 = 1- sum((zData.-DesignMatrix*beta).^2)/sum((zData .- z_avg).^2)\n",
    "    return R2\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    Bootstrap(xData, yData, zData, numDegrees::Int64, numBootstraps::Int64, Method; λ=0)\n",
    "\n",
    "Perform bootstrap resampling on the input data to estimate regression coefficients using a specified method (e.g., OLS, Ridge, or Lasso). Generates multiple bootstrap samples from the data, performs regression on each sample, and returns the corresponding coefficients.\n",
    "\n",
    "# Arguments\n",
    "- `xData::Array{Float64}`: x-coordinates of the input data.\n",
    "- `yData::Array{Float64}`: y-coordinates of the input data.\n",
    "- `zData::Array{Float64}`: Target values for regression (dependent variable).\n",
    "- `numDegrees::Int64`: Degree of the polynomial used to create the design matrix. This defines the model complexity.\n",
    "- `numBootstraps::Int64`: Number of bootstrap samples to generate.\n",
    "- `Method::Symbol`: Regression method to use. Options include:\n",
    "    - `:OLS` for Ordinary Least Squares.\n",
    "    - `:Ridge` for Ridge regression.\n",
    "    - `:Lasso` for Lasso regression.\n",
    "- `λ::Float64` (optional): Regularization parameter, used for Ridge or Lasso regression. Default is `0`.\n",
    "\n",
    "# Returns\n",
    "- `betas::Matrix{Float64}`: A matrix of size `(numCoefficients, numBootstraps)`, where each column contains the regression coefficients for a bootstrap sample.\n",
    "\n",
    "# Details\n",
    "1. **Bootstrap Sampling**: The function resamples the original data with replacement to create new datasets for each bootstrap iteration.\n",
    "2. **Design Matrix**: For each bootstrap sample, the design matrix is constructed using the `DataProjector` function, based on the specified polynomial degree.\n",
    "3. **Regression**: The specified regression method is applied on the bootstrap sample using the `Regression` function. If the method is Ridge or Lasso, regularization is controlled by the `λ` parameter.\n",
    "4. **Coefficient Storage**: The regression coefficients for each bootstrap sample are stored in the `betas` matrix.\n",
    "\n",
    "# Example\n",
    "```julia\n",
    "xData = rand(100)\n",
    "yData = rand(100)\n",
    "zData = rand(100)\n",
    "numDegrees = 5\n",
    "numBootstraps = 100\n",
    "Method = :OLS\n",
    "\n",
    "betas = Bootstrap(xData, yData, zData, numDegrees, numBootstraps, Method)\n",
    "\"\"\"\n",
    "function Bootstrap(xData, yData, zData, numDegrees::Int64, numBootstraps::Int64, Method; λ=0)\n",
    "    N = length(zData)\n",
    "    betas = zeros((numDegrees + 1) * (numDegrees + 2) ÷ 2,numBootstraps)\n",
    "    \n",
    "    for b in 1:numBootstraps\n",
    "        # Sample indices with replacement to create a bootstrap sample\n",
    "        bootstrap_indices = sample(1:N, N; replace=true)\n",
    "        \n",
    "        # Create bootstrap sample for xTrain, yTrain, and zTrain\n",
    "        xBootstrap = xData[bootstrap_indices]\n",
    "        yBootstrap = yData[bootstrap_indices]\n",
    "        zBootstrap = zData[bootstrap_indices]\n",
    "\n",
    "        # Create the design matrix for the bootstrap sample\n",
    "        DensityMatrixB_strap = DataProjector(xBootstrap, yBootstrap, zBootstrap, numDegrees)\n",
    "        \n",
    "        # Perform regression on the bootstrap sample\n",
    "        betaVals = Regression(DensityMatrixB_strap, zBootstrap, Method, λ=λ)\n",
    "        \n",
    "        # Store the regression coefficients\n",
    "        betas[:,b] .= betaVals\n",
    "    end\n",
    "\n",
    "    return betas\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    CrossValidation(X, Y, Z, k_folds::Int64, degrees::Int64, Method; lambda=0)\n",
    "\n",
    "Performs k-fold cross-validation to evaluate a regression model on the provided dataset. The function splits the data into `k_folds`, fits a model on the training data, and evaluates it on the test data for each fold, returning the mean error across all folds.\n",
    "\n",
    "# Arguments\n",
    "- `X::Array{Float64}`: The x-coordinates of the input data (for grid).\n",
    "- `Y::Array{Float64}`: The y-coordinates of the input data (for grid).\n",
    "- `Z::Array{Float64}`: The z-values (target) of the dataset.\n",
    "- `k_folds::Int64`: The number of folds to use in cross-validation. The data is split into `k_folds` parts, where each part is used once as the test set.\n",
    "- `degrees::Int64`: The degree of the polynomial used to construct the design matrix.\n",
    "- `Method::Symbol`: The regression method to use. Can be:\n",
    "    - `:OLS` for Ordinary Least Squares.\n",
    "    - `:Ridge` for Ridge regression.\n",
    "    - `:Lasso` for Lasso regression.\n",
    "- `lambda::Float64` (optional): The regularization parameter for Ridge or Lasso regression. Default is `0` (no regularization).\n",
    "\n",
    "# Returns\n",
    "- `mean_error::Float64`: The mean error (MSE) across all the k-folds of cross-validation.\n",
    "\n",
    "# Details\n",
    "1. **Data Processing**: The input data `X`, `Y`, and `Z` are flattened and reshaped to prepare for the regression model. A design matrix is constructed using the `DataProjector` function, mapping the inputs into a higher-dimensional polynomial space defined by `degrees`.\n",
    "2. **Shuffling and Splitting**: The data is shuffled randomly to remove any bias from its original ordering. The shuffled data is split into `k_folds` parts for cross-validation.\n",
    "3. **Cross-Validation Loop**: For each fold:\n",
    "    - The training set consists of all data except the current fold, and the test set consists of the current fold.\n",
    "    - A regression model is fitted on the training data.\n",
    "    - The model is evaluated on the test data, and the error (MSE) is calculated.\n",
    "4. **Error Calculation**: The mean squared error (MSE) is computed for each fold, and the average error across all folds is returned as the overall performance metric.\n",
    "\n",
    "# Example\n",
    "```julia\n",
    "X = rand(10)\n",
    "Y = rand(10)\n",
    "Z = rand(10, 10)\n",
    "k_folds = 5\n",
    "degrees = 3\n",
    "Method = :Ridge\n",
    "\n",
    "meanerror = CrossValidation(X, Y, Z, k_folds, degrees, Method, lambda=0.1)\n",
    "\n",
    "\"\"\"\n",
    "function CrossValidation(X, Y, Z, k_folds::Int64, degrees::Int64, Method;lambda=0)\n",
    "    #  ----  Data Processing  ----\n",
    "    xData = repeat(X, inner=length(X))  # Repeat xData \n",
    "    yData = repeat(Y, outer=length(Y))  # Repeat yData \n",
    "    zData = vec(Z) #Flattened Z data\n",
    "    DM = DataProjector(xData, yData, zData, degrees)\n",
    "    \n",
    "    N = length(zData)\n",
    "    split = Int(N/k_folds)\n",
    "    Errors = zeros(k_folds)\n",
    "    \n",
    "    #Shuffle\n",
    "    shuffle = sample(1:N,N,replace=false)\n",
    "    xData = xData[shuffle]\n",
    "    yData = yData[shuffle]\n",
    "    zData = zData[shuffle]\n",
    "    \n",
    "    for i in 1:k_folds\n",
    "        #  ----  Split Data  ----\n",
    "        a = split*(i-1)+1\n",
    "        b = split*(i)\n",
    "        \n",
    "        xTest = xData[a:b]\n",
    "        yTest = yData[a:b]\n",
    "        zTest = zData[a:b]\n",
    "        \n",
    "        xTrain = vcat(xData[1:a-1], xData[b+1:end])\n",
    "        yTrain = vcat(yData[1:a-1], yData[b+1:end])\n",
    "        zTrain = vcat(zData[1:a-1], zData[b+1:end])\n",
    "        \n",
    "        D_M_train = DataProjector(xTrain, yTrain, zTrain, degrees)\n",
    "        D_M_test = DataProjector(xTest, yTest, zTest, degrees)\n",
    "        \n",
    "        #  ----  Apply Regression  ----\n",
    "        betaVals = Regression(D_M_train, zTrain, Method, λ=lambda)\n",
    "        if Method==\"Lasso\"\n",
    "            mse=MSE(D_M_test,zTest,betaVals,λ = 0, γ = lambda)\n",
    "        else\n",
    "            mse = MSE(D_M_test,zTest,betaVals, λ = lambda, γ=0)\n",
    "        end\n",
    "        Errors[i] = mse\n",
    "    end\n",
    "    return mean(Errors)\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
