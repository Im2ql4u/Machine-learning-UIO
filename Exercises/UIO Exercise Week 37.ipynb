{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdecca8f",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Times New Roman; font-size:120%\">\n",
    "    -Aleksander Nistad Sekkelsten\n",
    "</p>\n",
    "<h1 style=\"font-family:Times New Roman; font-size: 300%\"><center> Machine Learning For Physics; Exercise Week 37 </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a065e0",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman; font-size:200%\">Exercise 1 |  Expectation values for ordinary least squares expressions |</h2>\n",
    "\n",
    "We will derive the expectation and variance of $y_i$ in the linear regression model:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "where the noise term:\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim N(0, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "We will also derive the expectation and variance of the ordinary least squares (OLS) estimator $\\hat{\\beta}$.\n",
    "\n",
    "### Expectation and Variance of $y_i$\n",
    "\n",
    "Given the model:\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\epsilon_i = X_i \\beta + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$ is normally distributed noise with mean 0 and variance $\\sigma^2$, we seek to determine the expectation and variance of $y_i$.\n",
    "\n",
    "#### Expectation of $y_i$\n",
    "\n",
    "The expectation of $y_i$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[y_i] = \\mathbb{E}[X_i \\beta + \\epsilon_i]\n",
    "$$\n",
    "\n",
    "Since $\\epsilon_i$ has zero mean, $\\mathbb{E}[\\epsilon_i] = 0$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[y_i] = \\mathbb{E}[X_i \\beta] + \\mathbb{E}[\\epsilon_i] = X_i \\beta\n",
    "$$\n",
    "\n",
    "Thus, the expectation of $y_i$ is $X_i \\beta$.\n",
    "\n",
    "#### Variance of $y_i$\n",
    "\n",
    "The variance of $y_i$ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y_i) = \\text{Var}(X_i \\beta + \\epsilon_i)\n",
    "$$\n",
    "\n",
    "Since $X_i \\beta$ is deterministic and has no variance, we are left with the variance of $\\epsilon_i$. Thus:\n",
    "\n",
    "$$\n",
    "\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n",
    "$$\n",
    "\n",
    "Therefore, the variance of $y_i$ is $\\sigma^2$.\n",
    "\n",
    "### Expectation and Variance of $\\hat{\\beta}$\n",
    "\n",
    "Now, we derive the expectation and variance of the OLS estimator $\\hat{\\beta}$. The OLS estimator is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Assuming the true relationship between $y$ and $X$ is:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2 I)$. Substituting this into the expression for $\\hat{\\beta}$, we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T (X\\beta + \\epsilon)\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T X \\beta + (X^T X)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "The term $(X^T X)^{-1} X^T X$ simplifies to the identity matrix, so:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\beta + (X^T X)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "#### Expectation of $\\hat{\\beta}$\n",
    "\n",
    "Taking the expectation of $\\hat{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = \\mathbb{E}[\\beta + (X^T X)^{-1} X^T \\epsilon]\n",
    "$$\n",
    "\n",
    "Since $\\beta$ is a constant and $\\mathbb{E}[\\epsilon] = 0$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}] = \\beta + (X^T X)^{-1} X^T \\mathbb{E}[\\epsilon] = \\beta\n",
    "$$\n",
    "\n",
    "Thus, the OLS estimator $\\hat{\\beta}$ is unbiased, meaning $\\mathbb{E}[\\hat{\\beta}] = \\beta$.\n",
    "\n",
    "#### Variance of $\\hat{\\beta}$\n",
    "\n",
    "Next, we compute the variance of $\\hat{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\text{Var}(\\beta + (X^T X)^{-1} X^T \\epsilon)\n",
    "$$\n",
    "\n",
    "Since $\\beta$ is a constant and has no variance, we focus on the second term:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\text{Var}((X^T X)^{-1} X^T \\epsilon)\n",
    "$$\n",
    "\n",
    "Using the fact that $\\text{Var}(A \\epsilon) = A \\cdot \\text{Var}(\\epsilon) \\cdot A^T$, we get:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = (X^T X)^{-1} X^T \\text{Var}(\\epsilon) X (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "Since $\\epsilon \\sim N(0, \\sigma^2 I)$, we have $\\text{Var}(\\epsilon) = \\sigma^2 I$, so:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n",
    "$$\n",
    "\n",
    "Thus, the variance of the OLS estimator $\\hat{\\beta}$ is $\\sigma^2 (X^T X)^{-1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5808f6",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman; font-size:200%\">Exercise 2 |  Expectation values for Ridge regression |</h2>\n",
    "\n",
    "\n",
    "### Expectation of $\\hat{\\beta}_{\\text{ridge}}$\n",
    "\n",
    "To find the expectation of $\\hat{\\beta}_{\\text{ridge}}$, we first substitute the model for $y$ into the ridge estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T (X\\beta + \\epsilon)\n",
    "$$\n",
    "\n",
    "Expanding this, we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T X \\beta + (X^T X + \\lambda I)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "Now, we take the expectation of both sides:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}] = \\mathbb{E}[(X^T X + \\lambda I)^{-1} X^T X \\beta] + \\mathbb{E}[(X^T X + \\lambda I)^{-1} X^T \\epsilon]\n",
    "$$\n",
    "\n",
    "Since $\\epsilon$ is zero-mean, $\\mathbb{E}[\\epsilon] = 0$, the second term disappears, and we are left with:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}] = (X^T X + \\lambda I)^{-1} X^T X \\beta\n",
    "$$\n",
    "\n",
    "This shows that the ridge regression estimator is **biased**. The bias depends on the regularization parameter $\\lambda$.\n",
    "\n",
    "### Variance of $\\hat{\\beta}_{\\text{ridge}}$\n",
    "\n",
    "Now let's compute the variance of $\\hat{\\beta}_{\\text{ridge}}$. From the expression for $\\hat{\\beta}_{\\text{ridge}}$, we know:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T X \\beta + (X^T X + \\lambda I)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "The variance depends only on the second term since the first term is deterministic. Therefore, we need to compute:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{\\text{ridge}}) = \\text{Var}((X^T X + \\lambda I)^{-1} X^T \\epsilon)\n",
    "$$\n",
    "\n",
    "Using the property $\\text{Var}(A \\epsilon) = A \\cdot \\text{Var}(\\epsilon) \\cdot A^T$, we have:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{\\text{ridge}}) = (X^T X + \\lambda I)^{-1} X^T \\text{Var}(\\epsilon) X (X^T X + \\lambda I)^{-1}\n",
    "$$\n",
    "\n",
    "Since $\\text{Var}(\\epsilon) = \\sigma^2 I$, we get:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{\\text{ridge}}) = \\sigma^2 (X^T X + \\lambda I)^{-1} X^T X (X^T X + \\lambda I)^{-1}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.8.5",
   "language": "julia",
   "name": "julia-_8-threads_-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
